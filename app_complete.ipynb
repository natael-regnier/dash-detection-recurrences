{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec206798-8ddf-44a5-be65-afd229f30f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import matplotlib.ticker as mticker\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, dash_table\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, State, dash_table\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.metrics import silhouette_score\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80150336-79b4-4259-ba0f-e0cdc7678e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m445296\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning:\n",
      "\n",
      "Workbook contains no default style, apply openpyxl's default\n",
      "\n",
      "C:\\Users\\m445296\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning:\n",
      "\n",
      "Workbook contains no default style, apply openpyxl's default\n",
      "\n",
      "C:\\Users\\m445296\\AppData\\Local\\Temp\\ipykernel_22340\\4012962304.py:534: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n",
      "C:\\Users\\m445296\\AppData\\Local\\Temp\\ipykernel_22340\\4012962304.py:263: UserWarning:\n",
      "\n",
      "Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2039be950f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.metrics import silhouette_score\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Pour l'application web (Dash et Plotly)\n",
    "import dash\n",
    "from dash import dcc, html, dash_table, Output, Input, State\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION DES FICHIERS ET DU TYPE D'AVION\n",
    "# =============================================================================\n",
    "ecam_a330 = \"Test RECURENCE ACARS AIB A330 avec dates_1948_1446609848899947727.xlsx\"\n",
    "ecam_b777 = \"B777 Faultrécurentes.xlsx\"\n",
    "ecam_a320 = \"Test RECURENCE ACARS AIB 320_2904_2442989602118074776.xlsx\"\n",
    "fichier_mtx = \"Pannes_Recurrentes_VBA.xlsm\"\n",
    "type_avion = \"B777\"  # Changez cette valeur (A330, A320, B777) selon le besoin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# =============================================================================\n",
    "def generate_date_columns(col_headers, day_numbers):\n",
    "    \"\"\"\n",
    "    Extrait les dates à partir des en‐têtes de colonnes et des numéros de jour.\n",
    "    Retourne une liste d'objets datetime correspondant aux dates calculées.\n",
    "    \"\"\"\n",
    "    new_date_cols = []\n",
    "    for header, day in zip(col_headers, day_numbers):\n",
    "        # Récupération de l'année (partie avant le point si existant)\n",
    "        if isinstance(header, (int, float)):\n",
    "            year_str = str(int(header))\n",
    "        else:\n",
    "            year_str = str(header).split('.')[0]\n",
    "        try:\n",
    "            year = int(year_str)\n",
    "        except Exception:\n",
    "            year = 1900  # Valeur par défaut en cas d'erreur\n",
    "        try:\n",
    "            day_int = int(day)\n",
    "        except Exception:\n",
    "            day_int = 1\n",
    "        date_val = datetime(year, 1, 1) + timedelta(days=day_int - 1)\n",
    "        new_date_cols.append(date_val)\n",
    "    return new_date_cols\n",
    "\n",
    "\n",
    "def clean_common_columns(df, col_names):\n",
    "    \"\"\"\n",
    "    Effectue le nettoyage commun sur les colonnes textuelles indiquées (suppression espaces).\n",
    "    \"\"\"\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAITEMENT DES FICHIERS EXCEL (Airbus & Boeing)\n",
    "# =============================================================================\n",
    "def process_sheet_new_format(file_path, sheet_name, swap_columns=False):\n",
    "    \"\"\"\n",
    "    Traite une feuille Excel au format \"nouveau\" pour Airbus.\n",
    "    La feuille possède trois premières colonnes (identifiants) et ensuite les colonnes de dates.\n",
    "    Si swap_columns est True, échange les deux premières colonnes (exemple : fiche \"60 J\").\n",
    "    \"\"\"\n",
    "    df_raw = pd.read_excel(file_path, sheet_name=sheet_name, header=0)\n",
    "    # Les colonnes de date commencent à partir de la 4ème colonne (index 3)\n",
    "    day_numbers = df_raw.iloc[0, 3:].tolist()\n",
    "    date_col_headers = df_raw.columns[3:]\n",
    "    new_date_cols = generate_date_columns(date_col_headers, day_numbers)\n",
    "    \n",
    "    # Supprimer la première ligne d'en-tête (utilisée pour les jours)\n",
    "    df_data = df_raw.iloc[1:, :].reset_index(drop=True)\n",
    "    \n",
    "    # Pour certaines feuilles, l'ordre des deux premières colonnes est inversé\n",
    "    if swap_columns:\n",
    "        cols = df_data.columns.tolist()\n",
    "        cols[0], cols[1] = cols[1], cols[0]\n",
    "        df_data = df_data[cols]\n",
    "    \n",
    "    # Renommer les colonnes (les trois premières puis les dates)\n",
    "    new_columns = [\"IMMAT\", \"ATA\", \"FAULT\"] + new_date_cols\n",
    "    df_data.columns = new_columns\n",
    "\n",
    "    # Remplir les valeurs manquantes dans les colonnes de dates par 0\n",
    "    df_data.iloc[:, 3:] = df_data.iloc[:, 3:].fillna(0)\n",
    "    df_data = clean_common_columns(df_data, [\"IMMAT\", \"FAULT\"])\n",
    "    df_data[\"ATA\"] = pd.to_numeric(df_data[\"ATA\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # Insertion de la colonne USEFUL\n",
    "    pannes_a_exclure = [\n",
    "        \"AUTO FLT A/THR OFF\",\n",
    "        \"AUTO FLT AP OFF\",\n",
    "        \"BRAKES HOT\",\n",
    "        \"SURV ROW/ROP LOST\",\n",
    "        \"NAV ALTI DISCREPANCY\",\n",
    "        \"NAV GPS 1 FAULT\",\n",
    "        \"NAV GPS 2 FAULT\",\n",
    "        \"NAV GPS1 FAULT\",\n",
    "        \"NAV GPS2 FAULT\"\n",
    "    ]\n",
    "    df_data.insert(3, 'USEFUL', ~df_data['FAULT'].isin(pannes_a_exclure))\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def process_airbus_excel_new_format(file_path):\n",
    "    \"\"\"\n",
    "    Traite un fichier Excel Airbus comportant les feuilles \"60 J\" et \"360 J\".\n",
    "    Retourne un tuple de DataFrames (60J, 360J) après transformation.\n",
    "    \"\"\"\n",
    "    df_60J = process_sheet_new_format(file_path, sheet_name=\"60 J\", swap_columns=True)\n",
    "    df_360J = process_sheet_new_format(file_path, sheet_name=\"360 J\", swap_columns=False)\n",
    "    return df_60J, df_360J\n",
    "\n",
    "\n",
    "def process_sheet_new_format_boeing(file_path, sheet_name):\n",
    "    \"\"\"\n",
    "    Traite une feuille Excel au format \"nouveau\" pour Boeing.\n",
    "    On supprime la première colonne et la feuille comporte ensuite IMMAT et FAULT,\n",
    "    puis les colonnes de dates.\n",
    "    \"\"\"\n",
    "    df_raw = pd.read_excel(file_path, sheet_name=sheet_name, header=0)\n",
    "    df_raw = df_raw.iloc[:, 1:]  # suppression de la première colonne\n",
    "\n",
    "    # Pour Boeing, les colonnes de dates commencent à l'indice 2\n",
    "    day_numbers = df_raw.iloc[0, 2:].tolist()\n",
    "    date_col_headers = df_raw.columns[2:]\n",
    "    new_date_cols = generate_date_columns(date_col_headers, day_numbers)\n",
    "\n",
    "    df_data = df_raw.iloc[1:, :].reset_index(drop=True)\n",
    "    new_columns = [\"IMMAT\", \"FAULT\"] + new_date_cols\n",
    "    df_data.columns = new_columns\n",
    "\n",
    "    df_data.iloc[:, 2:] = df_data.iloc[:, 2:].fillna(0)\n",
    "    df_data = clean_common_columns(df_data, [\"IMMAT\", \"FAULT\"])\n",
    "\n",
    "    # Insertion de la colonne USEFUL\n",
    "    pannes_a_exclure = [\n",
    "        \"AUTO FLT A/THR OFF\",\n",
    "        \"AUTO FLT AP OFF\",\n",
    "        \"BRAKES HOT\",\n",
    "        \"SURV ROW/ROP LOST\",\n",
    "        \"NAV ALTI DISCREPANCY\",\n",
    "        \"NAV GPS 1 FAULT\",\n",
    "        \"NAV GPS 2 FAULT\",\n",
    "        \"NAV GPS1 FAULT\",\n",
    "        \"NAV GPS2 FAULT\"\n",
    "    ]\n",
    "    df_data.insert(2, 'USEFUL', ~df_data['FAULT'].isin(pannes_a_exclure))\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "def process_boeing_excel_new_format(file_path):\n",
    "    \"\"\"\n",
    "    Traite un fichier Excel Boeing comportant les feuilles \"60 J\" et \"360 J\".\n",
    "    Retourne un tuple de DataFrames (60J, 360J) après transformation.\n",
    "    \"\"\"\n",
    "    df_60J = process_sheet_new_format_boeing(file_path, sheet_name=\"60 J\")\n",
    "    df_360J = process_sheet_new_format_boeing(file_path, sheet_name=\"360 J\")\n",
    "    return df_60J, df_360J\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTIONS DE FUSION DES DATAFRAMES\n",
    "# =============================================================================\n",
    "def merge_60j_into_360j_generic(df_60j, df_360j, key_columns, date_start_index):\n",
    "    \"\"\"\n",
    "    Fusionne les données d'un DataFrame 60J dans le DataFrame 360J.\n",
    "    \n",
    "    Paramètres\n",
    "    ----------\n",
    "    df_60j : pd.DataFrame\n",
    "        DataFrame contenant les données 60J\n",
    "    df_360j : pd.DataFrame\n",
    "        DataFrame cible (360J)\n",
    "    key_columns : list\n",
    "        Liste de colonnes identifiant de façon unique une ligne (ex. [\"IMMAT\", \"ATA\", \"FAULT\"] ou [\"IMMAT\", \"FAULT\"])\n",
    "    date_start_index : int\n",
    "        L'index à partir duquel les colonnes de date commencent dans df_360j\n",
    "    \n",
    "    Retourne\n",
    "    -------\n",
    "    df_360j : pd.DataFrame\n",
    "        DataFrame 360J mis à jour\n",
    "    \"\"\"\n",
    "    # Identification des colonnes de dates\n",
    "    date_cols_60j = df_60j.columns[date_start_index:]\n",
    "    date_cols_360j = df_360j.columns[date_start_index:]\n",
    "    \n",
    "    # Ajout des colonnes manquantes dans df_360j (remplies de 0)\n",
    "    missing_date_cols = [col for col in date_cols_60j if col not in date_cols_360j]\n",
    "    for col in missing_date_cols:\n",
    "        df_360j[col] = 0\n",
    "    \n",
    "    # Réordonner les colonnes de df_360j de façon chronologique\n",
    "    fixed_cols = df_360j.columns[:date_start_index]\n",
    "    dynamic_cols = df_360j.columns[date_start_index:]\n",
    "    sorted_date_cols = sorted(dynamic_cols, key=lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "    df_360j = df_360j[list(fixed_cols) + sorted_date_cols]\n",
    "    \n",
    "    # Fusion : pour chaque ligne de df_60j, mettre à jour la ligne correspondante dans df_360j ou l'ajouter\n",
    "    for _, row_60j in df_60j.iterrows():\n",
    "        mask = np.logical_and.reduce([df_360j[k] == row_60j[k] for k in key_columns])\n",
    "        matching_index = df_360j[mask].index\n",
    "\n",
    "        if matching_index.empty:\n",
    "            # Créer une nouvelle ligne\n",
    "            new_row = {}\n",
    "            # Recopie des colonnes fixes (si manquantes, on met 0 par défaut)\n",
    "            for col in fixed_cols:\n",
    "                new_row[col] = row_60j[col] if col in row_60j else 0\n",
    "            # Initialisation des colonnes de date à 0\n",
    "            for col in sorted_date_cols:\n",
    "                new_row[col] = 0\n",
    "            # Mise à jour avec les valeurs de date présentes dans 60J\n",
    "            for col in date_cols_60j:\n",
    "                new_row[col] = row_60j[col]\n",
    "            df_360j = pd.concat([df_360j, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        else:\n",
    "            # Mettre à jour les colonnes de date pour la ligne trouvée\n",
    "            for col in date_cols_60j:\n",
    "                df_360j.loc[matching_index, col] = row_60j[col]\n",
    "    \n",
    "    # Finaliser l'ordre des colonnes\n",
    "    df_360j = df_360j[list(fixed_cols) + sorted_date_cols]\n",
    "    return df_360j\n",
    "\n",
    "\n",
    "def merge_60j_into_360j(df_60J_airbus, df_360J_airbus):\n",
    "    \"\"\"\n",
    "    Fusionne pour Airbus en considérant comme clés [\"IMMAT\", \"ATA\", \"FAULT\"] et\n",
    "    en utilisant les colonnes de dates à partir de l'index 4.\n",
    "    \"\"\"\n",
    "    return merge_60j_into_360j_generic(df_60J_airbus, df_360J_airbus, key_columns=[\"IMMAT\", \"ATA\", \"FAULT\"], date_start_index=4)\n",
    "\n",
    "\n",
    "def merge_60j_into_360j_boeing(df_60J_boeing, df_360J_boeing):\n",
    "    \"\"\"\n",
    "    Fusionne pour Boeing en considérant comme clés [\"IMMAT\", \"FAULT\"] et\n",
    "    en utilisant les colonnes de dates à partir de l'index 3.\n",
    "    \"\"\"\n",
    "    return merge_60j_into_360j_generic(df_60J_boeing, df_360J_boeing, key_columns=[\"IMMAT\", \"FAULT\"], date_start_index=3)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FILTRAGE ET CLUSTERING DES LIGNES\n",
    "# =============================================================================\n",
    "def filter_rows_by_mean_gap_dates(df, facteur):\n",
    "    \"\"\"\n",
    "    Filtre les lignes de df en effectuant un clustering MeanShift\n",
    "    sur les offsets (en jours) des évènements par rapport à aujourd'hui.\n",
    "    Conserve la ligne si (facteur * intervalle_moyen > dernier_event).\n",
    "    Ajoute le \"Silhouette Score\" si possible.\n",
    "    \"\"\"\n",
    "    potential_dates = pd.to_datetime(df.columns, errors='coerce')\n",
    "    date_cols = df.columns[potential_dates.notna()]\n",
    "    today = pd.to_datetime(\"today\").normalize()\n",
    "    filtered_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        event_offsets = []\n",
    "        for col in date_cols:\n",
    "            if row[col] != 0:\n",
    "                col_date = pd.to_datetime(col)\n",
    "                event_offsets.append((today - col_date).days)\n",
    "        if not event_offsets:\n",
    "            continue\n",
    "        \n",
    "        X = np.array(event_offsets).reshape(-1, 1)\n",
    "        bandwidth = max(estimate_bandwidth(X, quantile=0.2, n_samples=len(X)), np.std(X) / 4, 1.0)\n",
    "        ms = MeanShift(bandwidth=bandwidth).fit(X)\n",
    "        clusters = ms.labels_\n",
    "        last_event_day = min(event_offsets)\n",
    "        # Identifier le cluster du dernier événement\n",
    "        last_cluster = clusters[np.where(X.flatten() == last_event_day)[0][0]]\n",
    "        cluster_days = [event_offsets[i] for i in range(len(event_offsets)) if clusters[i] == last_cluster]\n",
    "        mean_interval = np.mean(np.diff(sorted(cluster_days))) if len(cluster_days) >= 3 else 0\n",
    "        \n",
    "        if facteur * mean_interval > last_event_day:\n",
    "            #sil_score = silhouette_score(X, clusters) if len(np.unique(clusters)) > 1 else -1\n",
    "            row_copy = row.copy()\n",
    "            #row_copy[\"Silhouette Score\"] = sil_score\n",
    "            filtered_rows.append(row_copy)\n",
    "    \n",
    "    return pd.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CALCUL DU NOMBRE D'OCCURRENCES RÉCENTES\n",
    "# =============================================================================\n",
    "def compute_recent_occurrences(row):\n",
    "    \"\"\"\n",
    "    Pour une ligne, extrait les dates effectives et réalise un clustering MeanShift.\n",
    "    Retourne le nombre d'occurrences du dernier cluster (celui le plus récent).\n",
    "    \"\"\"\n",
    "    event_dates = []\n",
    "    for col in row.index:\n",
    "        dt = pd.to_datetime(col, format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "        if pd.notnull(dt) and row[col] != 0:\n",
    "            event_dates.append(dt)\n",
    "    if not event_dates:\n",
    "        return 0\n",
    "    event_dates.sort()\n",
    "    X = np.array([d.toordinal() for d in event_dates]).reshape(-1, 1)\n",
    "    bandwidth = max(estimate_bandwidth(X, quantile=0.2, n_samples=len(X)), np.std(X) / 4, 1.0)\n",
    "    clusters = MeanShift(bandwidth=bandwidth).fit_predict(X)\n",
    "    # Renumérotation des clusters\n",
    "    cluster_mapping = {}\n",
    "    clusters_new = []\n",
    "    current_label = 1\n",
    "    for c in clusters:\n",
    "        if c not in cluster_mapping:\n",
    "            cluster_mapping[c] = current_label\n",
    "            current_label += 1\n",
    "        clusters_new.append(cluster_mapping[c])\n",
    "    # Grouper les dates par cluster et choisir celui dont la date maximale est la plus récente\n",
    "    cluster_to_dates = {}\n",
    "    for d, cl in zip(event_dates, clusters_new):\n",
    "        cluster_to_dates.setdefault(cl, []).append(d)\n",
    "    last_cluster = max(cluster_to_dates, key=lambda cl: max(cluster_to_dates[cl]))\n",
    "    return clusters_new.count(last_cluster)\n",
    "\n",
    "\n",
    "def calculate_recent_occurrences(df):\n",
    "    \"\"\"\n",
    "    Ajoute à chaque ligne de df le nombre d'occurrences récentes (selon le dernier cluster)\n",
    "    et une échelle de couleur.\n",
    "    Retourne le DataFrame trié par occurrences décroissantes.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"Nombre d'occurrences récentes\"] = df.apply(compute_recent_occurrences, axis=1)\n",
    "    min_val = df[\"Nombre d'occurrences récentes\"].min()\n",
    "    max_val = df[\"Nombre d'occurrences récentes\"].max()\n",
    "    denom = (max_val - min_val) if max_val != min_val else 1\n",
    "    df[\"Color Scale\"] = (df[\"Nombre d'occurrences récentes\"] - min_val) / denom\n",
    "    columns_to_return = [\"IMMAT\", \"FAULT\", \"Nombre d'occurrences récentes\", \"Color Scale\"]\n",
    "    if \"ATA\" in df.columns:\n",
    "        columns_to_return.insert(2, \"ATA\")\n",
    "    df = df[columns_to_return].drop_duplicates().sort_values(by=\"Nombre d'occurrences récentes\", ascending=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTIONS DE PLOTTING ET STYLE\n",
    "# =============================================================================\n",
    "def get_contrasting_text_color(color_str):\n",
    "    \"\"\"\n",
    "    Retourne 'black' ou 'white' en fonction de la luminosité de la couleur donnée au format hex ou rgb.\n",
    "    \"\"\"\n",
    "    if color_str.startswith('#'):\n",
    "        hex_color = color_str.lstrip('#')\n",
    "        r = int(hex_color[0:2], 16)\n",
    "        g = int(hex_color[2:4], 16)\n",
    "        b = int(hex_color[4:6], 16)\n",
    "    elif color_str.startswith('rgb'):\n",
    "        inner = color_str[color_str.find('(')+1:color_str.find(')')]\n",
    "        r, g, b = [int(p.strip()) for p in inner.split(',')]\n",
    "    else:\n",
    "        r, g, b = (0, 0, 0)\n",
    "    luminance = (0.299 * r + 0.587 * g + 0.114 * b)\n",
    "    return 'black' if luminance > 186 else 'white'\n",
    "\n",
    "\n",
    "def plot_timeline_with_clusters_meanshift_plotly(df, immat=None, ata=None, fault=None):\n",
    "    global df_result\n",
    "\n",
    "    # Filtrer la ligne sélectionnée selon (IMMAT, ATA, FAULT)\n",
    "    if \"ATA\" in df.columns:\n",
    "        df_selected = df[(df[\"IMMAT\"] == immat) & (df[\"ATA\"] == ata) & (df[\"FAULT\"] == fault)]\n",
    "    else:\n",
    "        df_selected = df[(df[\"IMMAT\"] == immat) & (df[\"FAULT\"] == fault)]\n",
    "        \n",
    "    if df_selected.empty:\n",
    "        return go.Figure(), pd.DataFrame()\n",
    "    row = df_selected.iloc[0]\n",
    "\n",
    "    # Identifier les colonnes de dates (en convertissant les noms de colonnes en datetime)\n",
    "    potential_dates = pd.to_datetime(df.columns, format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "    date_cols = df.columns[potential_dates.notna()]\n",
    "\n",
    "    # Récupérer les dates effectives à partir de la ligne sélectionnée\n",
    "    event_dates = []\n",
    "    for col in date_cols:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val != 0:\n",
    "            dt = pd.to_datetime(col, format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "            if pd.notnull(dt):\n",
    "                event_dates.append(dt)\n",
    "    if not event_dates:\n",
    "        return go.Figure(), pd.DataFrame()\n",
    "\n",
    "    event_dates.sort()\n",
    "    # Conversion des dates en entier (ordinal)\n",
    "    X = np.array([d.toordinal() for d in event_dates]).reshape(-1, 1)\n",
    "    bw_est = estimate_bandwidth(X, quantile=0.2, n_samples=len(X))\n",
    "    bandwidth = max(bw_est, np.std(X) / 4, 1.0)\n",
    "    clusters_raw = MeanShift(bandwidth=bandwidth).fit_predict(X)\n",
    "    \n",
    "    # Renumérotation des clusters pour commencer à 1\n",
    "    cluster_mapping = {}\n",
    "    clusters = []\n",
    "    current_label = 1\n",
    "    for c in clusters_raw:\n",
    "        if c not in cluster_mapping:\n",
    "            cluster_mapping[c] = current_label\n",
    "            current_label += 1\n",
    "        clusters.append(cluster_mapping[c])\n",
    "    \n",
    "    cumulative_faults = np.arange(1, len(event_dates) + 1)\n",
    "    points_by_cluster = {}\n",
    "    for d, cum, cl in zip(event_dates, cumulative_faults, clusters):\n",
    "        points_by_cluster.setdefault(cl, {\"x\": [], \"y\": []})\n",
    "        points_by_cluster[cl][\"x\"].append(d)\n",
    "        points_by_cluster[cl][\"y\"].append(cum)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    unique_clusters = sorted(points_by_cluster.keys())\n",
    "    base_palette = px.colors.qualitative.Set1\n",
    "    for i, cl in enumerate(unique_clusters):\n",
    "        data_clust = points_by_cluster[cl]\n",
    "        color = base_palette[i % len(base_palette)]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=data_clust[\"x\"],\n",
    "            y=data_clust[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=color, size=10),\n",
    "            name=f\"Cluster {cl}\"\n",
    "        ))\n",
    "    \n",
    "    # Trace fictif pour la légende des plaintes MTX\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode='lines',\n",
    "        line=dict(color='black', dash='dot'),\n",
    "        name='Plainte MTX'\n",
    "    ))\n",
    "    \n",
    "    # Récupérer dans df_result les lignes correspondantes\n",
    "    fault_cleaned = str(fault).replace(\" \", \"\")\n",
    "    if \"ATA\" in df_result.columns:\n",
    "        matching_df_result = df_result[\n",
    "            (df_result[\"IMMAT\"] == immat) &\n",
    "            ((df_result[\"ATA\"] == ata) |\n",
    "             (df_result[\"FAULT\"].astype(str).str.replace(\" \", \"\").str.contains(fault_cleaned, na=False)))\n",
    "        ].copy()\n",
    "    else:\n",
    "        matching_df_result = df_result[\n",
    "            (df_result[\"IMMAT\"] == immat) &\n",
    "            (df_result[\"FAULT\"].astype(str).str.replace(\" \", \"\").str.contains(fault_cleaned, na=False))\n",
    "        ].copy()\n",
    "    \n",
    "    matching_df_result['FoundDateDT'] = pd.to_datetime(matching_df_result[\"FoundDate\"],\n",
    "                                                        format=\"%d/%m/%Y %H:%M\", errors='coerce')\n",
    "    matching_df_result = matching_df_result.sort_values(by=\"FoundDateDT\", ascending=False).reset_index(drop=True)\n",
    "    matching_df_result.drop(columns=\"FoundDateDT\", inplace=True)\n",
    "    \n",
    "    # ASSIGNATION DES COULEURS POUR \"LineColor\"\n",
    "    color_palette = px.colors.qualitative.Set1\n",
    "    matching_df_result[\"LineColor\"] = [color_palette[i % len(color_palette)] for i in range(len(matching_df_result))]\n",
    "    \n",
    "    # Ajouter une trace Scatter pour chaque plainte MTX\n",
    "    for idx, row_res in matching_df_result.iterrows():\n",
    "        fd = pd.to_datetime(row_res[\"FoundDate\"], format=\"%d/%m/%Y %H:%M\", errors='coerce')\n",
    "        if pd.notna(fd):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[fd, fd],\n",
    "                y=[0, max(cumulative_faults)],\n",
    "                mode=\"lines+markers\",\n",
    "                line=dict(color=row_res[\"LineColor\"], dash=\"dot\"),\n",
    "                marker=dict(size=20, opacity=0),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=f\"Date: {fd.strftime('%d/%m/%Y %H:%M')}\",\n",
    "                showlegend=False\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Timeline des occurrences\",\n",
    "        yaxis_title=\"Numéro de l'occurrence\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig.update_xaxes(type=\"date\")\n",
    "    \n",
    "    table_df = matching_df_result[[\"FAULT\", \"FoundDate\", \"Tasks_Barcode\"]]\n",
    "    return fig, table_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STYLE ET OUTILS POUR LE DASHBOARD\n",
    "# =============================================================================\n",
    "def generate_row_styles(data):\n",
    "    \"\"\"\n",
    "    Applique un dégradé de rouge selon la \"Color Scale\" pour le style du tableau.\n",
    "    \"\"\"\n",
    "    styles = []\n",
    "    for i, row in enumerate(data):\n",
    "        intensity = int(255 * (1 - row[\"Color Scale\"]))\n",
    "        color = f\"rgb(255, {intensity}, {intensity})\"\n",
    "        styles.append({\n",
    "            \"if\": {\"row_index\": i},\n",
    "            \"backgroundColor\": color,\n",
    "            \"color\": \"black\"\n",
    "        })\n",
    "    return styles\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAITEMENT DES DONNÉES INITIALES\n",
    "# =============================================================================\n",
    "if type_avion == \"A330\":\n",
    "    df_60J, df_360J = process_airbus_excel_new_format(ecam_a330)\n",
    "    df_combined = merge_60j_into_360j(df_60J, df_360J)\n",
    "    df = df_combined.drop_duplicates()\n",
    "\n",
    "elif type_avion == \"A320\":\n",
    "    df_60J, df_360J = process_airbus_excel_new_format(ecam_a320)\n",
    "    df_combined = merge_60j_into_360j(df_60J, df_360J)\n",
    "    df = df_combined.drop_duplicates()\n",
    "\n",
    "elif type_avion == \"B777\":\n",
    "    df_60J, df_360J = process_boeing_excel_new_format(ecam_b777)\n",
    "    df_combined = merge_60j_into_360j_boeing(df_60J, df_360J)\n",
    "    df = df_combined.drop_duplicates()\n",
    "\n",
    "# Identification des colonnes qui représentent des dates\n",
    "potential_dates = pd.to_datetime(df.columns, errors='coerce')\n",
    "date_cols = df.columns[potential_dates.notna()]\n",
    "\n",
    "# Filtrage final : conserver uniquement les lignes avec au moins 3 évènements et dont USEFUL est True\n",
    "df_filtre = df[(df[date_cols].ne(0).sum(axis=1) >= 3) & (df[\"USEFUL\"] == True)]\n",
    "\n",
    "# Appliquer le filtrage par gap moyen\n",
    "df_final = filter_rows_by_mean_gap_dates(df_filtre, facteur=2)\n",
    "\n",
    "# Lecture et préparation des données MTX\n",
    "df_mtx = pd.read_excel(fichier_mtx, sheet_name=\"Data\")\n",
    "for col in df_mtx.select_dtypes(include=['object']):\n",
    "    df_mtx[col] = df_mtx[col].str.strip()\n",
    "df_mtx[\"ATA\"] = pd.to_numeric(df_mtx[\"ATA\"], errors=\"coerce\")\n",
    "df_mtx[\"ATA\"] = df_mtx[\"ATA\"].astype(\"Int64\")  # Option A pour garder <NA>\n",
    "df_mtx = df_mtx.rename(columns={\"Tasks_Aircraft\": \"IMMAT\", \"Task_Name_Only\": \"FAULT\"})\n",
    "df_result = df_mtx.copy()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION DU DASHBOARD AVEC DASH\n",
    "# =============================================================================\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Préparation du tableau principal\n",
    "filtered_df = calculate_recent_occurrences(df_final)\n",
    "faults = sorted(filtered_df[\"FAULT\"].unique())\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Détection de récurrences\"),\n",
    "    html.Div([\n",
    "        # Colonne gauche : menu et tableau principal\n",
    "        html.Div([\n",
    "            html.Details([\n",
    "                html.Summary(\"Sélectionner les faults\"),\n",
    "                # Ajout de la case à cocher pour retirer les pannes concernant plus de 3 avions\n",
    "                dcc.Checklist(\n",
    "                    id=\"remove-faults-option\",\n",
    "                    options=[{\"label\": \"Retirer les pannes concernant plus de 3 avions\", \"value\": \"remove\"}],\n",
    "                    value=[\"remove\"],  # Par défaut, cette case est cochée\n",
    "                    labelStyle={\"display\": \"block\"}\n",
    "                ),\n",
    "                html.Div([\n",
    "                    html.Button(\"Tout cocher\", id=\"select-all\", n_clicks=0),\n",
    "                    html.Button(\"Tout décocher\", id=\"deselect-all\", n_clicks=0)\n",
    "                ], style={'display': 'flex', 'gap': '10px', 'margin': '5px 0'}),\n",
    "                dcc.Checklist(\n",
    "                    id=\"fault-filter\",\n",
    "                    options=[{\"label\": f, \"value\": f} for f in faults],\n",
    "                    value=faults,\n",
    "                    labelStyle={\"display\": \"block\"}\n",
    "                )\n",
    "            ], open=False),\n",
    "\n",
    "            dash_table.DataTable(\n",
    "                id='table',\n",
    "                columns=[{'name': c, 'id': c} for c in filtered_df.columns if c != \"Color Scale\"],\n",
    "                data=filtered_df.to_dict('records'),\n",
    "                filter_action='native',\n",
    "                sort_action='native',\n",
    "                row_selectable='single',\n",
    "                style_table={'overflowX': 'auto', 'maxHeight': '600px', 'overflowY': 'scroll'},\n",
    "                style_data_conditional=generate_row_styles(filtered_df.to_dict('records'))\n",
    "            )\n",
    "        ], style={'width': '30%', 'padding': '10px', 'overflowY': 'auto'}),\n",
    "        # Colonne droite : graphique et tableau des plaintes MTX\n",
    "        html.Div([\n",
    "            dcc.Graph(id='timeline-plot'),\n",
    "            html.Hr(),\n",
    "            html.H3(\"Plaintes MTX\"),\n",
    "            dash_table.DataTable(\n",
    "                id='df-result-table',\n",
    "                columns=[{'name': c, 'id': c} for c in [\"FAULT\", \"FoundDate\", \"Tasks_Barcode\"]],\n",
    "                data=[],  # mis à jour par callback\n",
    "                style_table={'overflowX': 'auto', 'maxHeight': '300px', 'overflowY': 'scroll'},\n",
    "                style_cell={'textAlign': 'left'},\n",
    "            )\n",
    "        ], style={'width': '70%', 'padding': '10px'})\n",
    "    ], style={'display': 'flex', 'flexDirection': 'row'})\n",
    "])\n",
    "\n",
    "# Callback unifiée qui met à jour à la fois le tableau et la checklist des faults\n",
    "@app.callback(\n",
    "    [Output('table', 'data'),\n",
    "     Output('table', 'style_data_conditional'),\n",
    "     Output('fault-filter', 'options'),\n",
    "     Output('fault-filter', 'value')],\n",
    "    [Input('fault-filter', 'value'),\n",
    "     Input('remove-faults-option', 'value'),\n",
    "     Input('select-all', 'n_clicks'),\n",
    "     Input('deselect-all', 'n_clicks')],\n",
    "    State('fault-filter', 'options')\n",
    ")\n",
    "def update_table_and_faults(selected_faults, remove_option, select_all, deselect_all, current_options):\n",
    "    ctx = dash.callback_context\n",
    "    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None\n",
    "\n",
    "    # Partir du DataFrame initial (filtered_df)\n",
    "    df_table = filtered_df.copy()\n",
    "    \n",
    "    # Appliquer le filtrage si l'option \"Retirer les pannes concernant plus de 3 avions\" est activée\n",
    "    if \"remove\" in (remove_option or []):\n",
    "        fault_counts = df_table.groupby(\"FAULT\")[\"IMMAT\"].nunique()\n",
    "        faults_to_exclude = fault_counts[fault_counts > 3].index\n",
    "        df_table = df_table[~df_table[\"FAULT\"].isin(faults_to_exclude)]\n",
    "    \n",
    "    # Calculer la liste des faults disponibles pour la checklist\n",
    "    available_faults = sorted(df_table[\"FAULT\"].unique())\n",
    "    new_options = [{\"label\": f, \"value\": f} for f in available_faults]\n",
    "    \n",
    "    # Déterminer la sélection en fonction de l'action déclenchante\n",
    "    if trigger_id == 'select-all':\n",
    "        new_selected = available_faults\n",
    "    elif trigger_id == 'deselect-all':\n",
    "        new_selected = []\n",
    "    else:\n",
    "        # Conserver uniquement les valeurs qui apparaissent dans la liste disponible\n",
    "        new_selected = [f for f in (selected_faults or []) if f in available_faults]\n",
    "        if not new_selected:\n",
    "            new_selected = available_faults\n",
    "\n",
    "    # Filtrer le DataFrame selon les faults sélectionnées\n",
    "    if new_selected:\n",
    "        df_table = df_table[df_table[\"FAULT\"].isin(new_selected)]\n",
    "    else:\n",
    "        df_table = pd.DataFrame(columns=filtered_df.columns)\n",
    "    \n",
    "    records = df_table.to_dict('records')\n",
    "    new_styles = generate_row_styles(records)\n",
    "    return records, new_styles, new_options, new_selected\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [Output('timeline-plot', 'figure'),\n",
    "     Output('df-result-table', 'data'),\n",
    "     Output('df-result-table', 'style_data_conditional')],\n",
    "    [Input('table', 'selected_rows'),\n",
    "     Input('timeline-plot', 'restyleData')],\n",
    "    [State('table', 'data'),\n",
    "     State('timeline-plot', 'figure')]\n",
    ")\n",
    "def display_plot(selected_rows, restyleData, table_data, current_fig):\n",
    "    ctx = dash.callback_context\n",
    "    if not ctx.triggered:\n",
    "        raise dash.exceptions.PreventUpdate\n",
    "    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0]\n",
    "\n",
    "    if trigger_id == 'table':\n",
    "        if not selected_rows:\n",
    "            return go.Figure(), [], []\n",
    "        selected_row = table_data[selected_rows[0]]\n",
    "        immat = selected_row[\"IMMAT\"]\n",
    "        ata = selected_row.get(\"ATA\", None)\n",
    "        fault = selected_row[\"FAULT\"]\n",
    "        new_fig, result_df = plot_timeline_with_clusters_meanshift_plotly(df_final, immat=immat, ata=ata, fault=fault)\n",
    "        \n",
    "        result_df['FoundDateDT'] = pd.to_datetime(result_df['FoundDate'], format=\"%d/%m/%Y %H:%M\", errors='coerce')\n",
    "        result_df = result_df.sort_values(by='FoundDateDT', ascending=False)\n",
    "        result_df.drop(columns='FoundDateDT', inplace=True)\n",
    "        table_result_data = result_df.to_dict('records')\n",
    "        \n",
    "        style_result = []\n",
    "        color_palette = px.colors.qualitative.Set1\n",
    "        for i in range(len(result_df)):\n",
    "            bg = color_palette[i % len(color_palette)]\n",
    "            txt = get_contrasting_text_color(bg)\n",
    "            style_result.append({\n",
    "                'if': {'row_index': i},\n",
    "                'backgroundColor': bg,\n",
    "                'color': txt\n",
    "            })\n",
    "        return new_fig, table_result_data, style_result\n",
    "\n",
    "    elif trigger_id == 'timeline-plot' and restyleData is not None:\n",
    "        update_info = restyleData[0]\n",
    "        trace_indices = restyleData[1]\n",
    "        if 'visible' in update_info:\n",
    "            new_visible = update_info['visible'][0]\n",
    "            if new_visible != 'legendonly':\n",
    "                idx = trace_indices[0]\n",
    "                if current_fig and 'data' in current_fig and len(current_fig['data']) > idx:\n",
    "                    x_data = current_fig['data'][idx].get('x', [])\n",
    "                    parsed_dates = []\n",
    "                    for val in x_data:\n",
    "                        try:\n",
    "                            dt = parse(val)\n",
    "                            parsed_dates.append(dt)\n",
    "                        except:\n",
    "                            pass\n",
    "                    if parsed_dates:\n",
    "                        x_min = min(parsed_dates)\n",
    "                        x_max = max(parsed_dates)\n",
    "                        delta_days = (x_max - x_min).days\n",
    "                        x_margin_days = delta_days * 0.1 if delta_days > 0 else 1\n",
    "                        current_fig['layout']['xaxis']['range'] = [\n",
    "                            (x_min - pd.Timedelta(days=x_margin_days)).isoformat(),\n",
    "                            (x_max + pd.Timedelta(days=x_margin_days)).isoformat()\n",
    "                        ]\n",
    "        return current_fig, dash.no_update, dash.no_update\n",
    "    else:\n",
    "        return current_fig, dash.no_update, dash.no_update\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9346c45b-0757-43b5-bb8f-5cc115f4cc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1eef28b27b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.metrics import silhouette_score\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Pour l'application web (Dash et Plotly)\n",
    "import dash\n",
    "from dash import dcc, html, dash_table, Output, Input, State\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION DE L'APPLICATION\n",
    "# =============================================================================\n",
    "app = dash.Dash(__name__, suppress_callback_exceptions=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTIONS UTILITAIRES ET DE TRAITEMENT DES DONNÉES\n",
    "# =============================================================================\n",
    "def generate_date_columns(col_headers, day_numbers):\n",
    "    new_date_cols = []\n",
    "    for header, day in zip(col_headers, day_numbers):\n",
    "        if isinstance(header, (int, float)):\n",
    "            year_str = str(int(header))\n",
    "        else:\n",
    "            year_str = str(header).split('.')[0]\n",
    "        try:\n",
    "            year = int(year_str)\n",
    "        except Exception:\n",
    "            year = 1900\n",
    "        try:\n",
    "            day_int = int(day)\n",
    "        except Exception:\n",
    "            day_int = 1\n",
    "        date_val = datetime(year, 1, 1) + timedelta(days=day_int - 1)\n",
    "        new_date_cols.append(date_val)\n",
    "    return new_date_cols\n",
    "\n",
    "def clean_common_columns(df, col_names):\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# TRAITEMENT DES FICHIERS EXCEL (Airbus & Boeing)\n",
    "# =============================================================================\n",
    "def process_sheet_new_format(file_path, sheet_name, swap_columns=False):\n",
    "    df_raw = pd.read_excel(file_path, sheet_name=sheet_name, header=0)\n",
    "    day_numbers = df_raw.iloc[0, 3:].tolist()\n",
    "    date_col_headers = df_raw.columns[3:]\n",
    "    new_date_cols = generate_date_columns(date_col_headers, day_numbers)\n",
    "    \n",
    "    df_data = df_raw.iloc[1:, :].reset_index(drop=True)\n",
    "    if swap_columns:\n",
    "        cols = df_data.columns.tolist()\n",
    "        cols[0], cols[1] = cols[1], cols[0]\n",
    "        df_data = df_data[cols]\n",
    "    \n",
    "    new_columns = [\"IMMAT\", \"ATA\", \"FAULT\"] + new_date_cols\n",
    "    df_data.columns = new_columns\n",
    "    df_data.iloc[:, 3:] = df_data.iloc[:, 3:].fillna(0)\n",
    "    df_data = clean_common_columns(df_data, [\"IMMAT\", \"FAULT\"])\n",
    "    df_data[\"ATA\"] = pd.to_numeric(df_data[\"ATA\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    pannes_a_exclure = [\n",
    "        \"AUTO FLT A/THR OFF\", \"AUTO FLT AP OFF\", \"BRAKES HOT\",\n",
    "        \"SURV ROW/ROP LOST\", \"NAV ALTI DISCREPANCY\",\n",
    "        \"NAV GPS 1 FAULT\", \"NAV GPS 2 FAULT\", \"NAV GPS1 FAULT\", \"NAV GPS2 FAULT\"\n",
    "    ]\n",
    "    df_data.insert(3, 'USEFUL', ~df_data['FAULT'].isin(pannes_a_exclure))\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "def process_airbus_excel_new_format(file_path):\n",
    "    df_60J = process_sheet_new_format(file_path, sheet_name=\"60 J\", swap_columns=True)\n",
    "    df_360J = process_sheet_new_format(file_path, sheet_name=\"360 J\", swap_columns=False)\n",
    "    return df_60J, df_360J\n",
    "\n",
    "def process_sheet_new_format_boeing(file_path, sheet_name):\n",
    "    df_raw = pd.read_excel(file_path, sheet_name=sheet_name, header=0)\n",
    "    df_raw = df_raw.iloc[:, 1:]\n",
    "    day_numbers = df_raw.iloc[0, 2:].tolist()\n",
    "    date_col_headers = df_raw.columns[2:]\n",
    "    new_date_cols = generate_date_columns(date_col_headers, day_numbers)\n",
    "\n",
    "    df_data = df_raw.iloc[1:, :].reset_index(drop=True)\n",
    "    new_columns = [\"IMMAT\", \"FAULT\"] + new_date_cols\n",
    "    df_data.columns = new_columns\n",
    "    df_data.iloc[:, 2:] = df_data.iloc[:, 2:].fillna(0)\n",
    "    df_data = clean_common_columns(df_data, [\"IMMAT\", \"FAULT\"])\n",
    "    \n",
    "    pannes_a_exclure = [\n",
    "        \"AUTO FLT A/THR OFF\", \"AUTO FLT AP OFF\", \"BRAKES HOT\",\n",
    "        \"SURV ROW/ROP LOST\", \"NAV ALTI DISCREPANCY\",\n",
    "        \"NAV GPS 1 FAULT\", \"NAV GPS 2 FAULT\", \"NAV GPS1 FAULT\", \"NAV GPS2 FAULT\"\n",
    "    ]\n",
    "    df_data.insert(2, 'USEFUL', ~df_data['FAULT'].isin(pannes_a_exclure))\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "def process_boeing_excel_new_format(file_path):\n",
    "    df_60J = process_sheet_new_format_boeing(file_path, sheet_name=\"60 J\")\n",
    "    df_360J = process_sheet_new_format_boeing(file_path, sheet_name=\"360 J\")\n",
    "    return df_60J, df_360J\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTIONS DE FUSION DES DATAFRAMES\n",
    "# =============================================================================\n",
    "def merge_60j_into_360j_generic(df_60j, df_360j, key_columns, date_start_index):\n",
    "    date_cols_60j = df_60j.columns[date_start_index:]\n",
    "    date_cols_360j = df_360j.columns[date_start_index:]\n",
    "    missing_date_cols = [col for col in date_cols_60j if col not in date_cols_360j]\n",
    "    for col in missing_date_cols:\n",
    "        df_360j[col] = 0\n",
    "    fixed_cols = df_360j.columns[:date_start_index]\n",
    "    dynamic_cols = df_360j.columns[date_start_index:]\n",
    "    sorted_date_cols = sorted(dynamic_cols, key=lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "    df_360j = df_360j[list(fixed_cols) + sorted_date_cols]\n",
    "    for _, row_60j in df_60j.iterrows():\n",
    "        mask = np.logical_and.reduce([df_360j[k] == row_60j[k] for k in key_columns])\n",
    "        matching_index = df_360j[mask].index\n",
    "        if matching_index.empty:\n",
    "            new_row = {}\n",
    "            for col in fixed_cols:\n",
    "                new_row[col] = row_60j[col] if col in row_60j else 0\n",
    "            for col in sorted_date_cols:\n",
    "                new_row[col] = 0\n",
    "            for col in date_cols_60j:\n",
    "                new_row[col] = row_60j[col]\n",
    "            df_360j = pd.concat([df_360j, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        else:\n",
    "            for col in date_cols_60j:\n",
    "                df_360j.loc[matching_index, col] = row_60j[col]\n",
    "    df_360j = df_360j[list(fixed_cols) + sorted_date_cols]\n",
    "    return df_360j\n",
    "\n",
    "def merge_60j_into_360j(df_60J_airbus, df_360J_airbus):\n",
    "    return merge_60j_into_360j_generic(df_60J_airbus, df_360J_airbus, key_columns=[\"IMMAT\", \"ATA\", \"FAULT\"], date_start_index=4)\n",
    "\n",
    "def merge_60j_into_360j_boeing(df_60J_boeing, df_360J_boeing):\n",
    "    return merge_60j_into_360j_generic(df_60J_boeing, df_360J_boeing, key_columns=[\"IMMAT\", \"FAULT\"], date_start_index=3)\n",
    "\n",
    "# =============================================================================\n",
    "# FILTRAGE ET CLUSTERING DES LIGNES\n",
    "# =============================================================================\n",
    "def filter_rows_by_mean_gap_dates(df, facteur):\n",
    "    potential_dates = pd.to_datetime(df.columns, errors='coerce')\n",
    "    date_cols = df.columns[potential_dates.notna()]\n",
    "    today = pd.to_datetime(\"today\").normalize()\n",
    "    filtered_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        event_offsets = []\n",
    "        for col in date_cols:\n",
    "            if row[col] != 0:\n",
    "                col_date = pd.to_datetime(col)\n",
    "                event_offsets.append((today - col_date).days)\n",
    "        if not event_offsets:\n",
    "            continue\n",
    "        X = np.array(event_offsets).reshape(-1, 1)\n",
    "        bandwidth = max(estimate_bandwidth(X, quantile=0.2, n_samples=len(X)), np.std(X)/4, 1.0)\n",
    "        ms = MeanShift(bandwidth=bandwidth).fit(X)\n",
    "        clusters = ms.labels_\n",
    "        last_event_day = min(event_offsets)\n",
    "        last_cluster = clusters[np.where(X.flatten() == last_event_day)[0][0]]\n",
    "        cluster_days = [event_offsets[i] for i in range(len(event_offsets)) if clusters[i] == last_cluster]\n",
    "        mean_interval = np.mean(np.diff(sorted(cluster_days))) if len(cluster_days) >= 3 else 0\n",
    "        if facteur * mean_interval > last_event_day:\n",
    "            filtered_rows.append(row.copy())\n",
    "    return pd.DataFrame(filtered_rows)\n",
    "\n",
    "# =============================================================================\n",
    "# CALCUL DU NOMBRE D'OCCURRENCES RÉCENTES\n",
    "# =============================================================================\n",
    "def compute_recent_occurrences(row):\n",
    "    event_dates = []\n",
    "    for col in row.index:\n",
    "        dt = pd.to_datetime(col, format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "        if pd.notnull(dt) and row[col] != 0:\n",
    "            event_dates.append(dt)\n",
    "    if not event_dates:\n",
    "        return 0\n",
    "    event_dates.sort()\n",
    "    X = np.array([d.toordinal() for d in event_dates]).reshape(-1, 1)\n",
    "    bandwidth = max(estimate_bandwidth(X, quantile=0.2, n_samples=len(X)), np.std(X)/4, 1.0)\n",
    "    clusters = MeanShift(bandwidth=bandwidth).fit_predict(X)\n",
    "    cluster_mapping = {}\n",
    "    clusters_new = []\n",
    "    current_label = 1\n",
    "    for c in clusters:\n",
    "        if c not in cluster_mapping:\n",
    "            cluster_mapping[c] = current_label\n",
    "            current_label += 1\n",
    "        clusters_new.append(cluster_mapping[c])\n",
    "    cluster_to_dates = {}\n",
    "    for d, cl in zip(event_dates, clusters_new):\n",
    "        cluster_to_dates.setdefault(cl, []).append(d)\n",
    "    last_cluster = max(cluster_to_dates, key=lambda cl: max(cluster_to_dates[cl]))\n",
    "    return clusters_new.count(last_cluster)\n",
    "\n",
    "def calculate_recent_occurrences(df):\n",
    "    df = df.copy()\n",
    "    df[\"Nombre d'occurrences récentes\"] = df.apply(compute_recent_occurrences, axis=1)\n",
    "    min_val = df[\"Nombre d'occurrences récentes\"].min()\n",
    "    max_val = df[\"Nombre d'occurrences récentes\"].max()\n",
    "    denom = (max_val - min_val) if max_val != min_val else 1\n",
    "    df[\"Color Scale\"] = (df[\"Nombre d'occurrences récentes\"] - min_val) / denom\n",
    "    columns_to_return = [\"IMMAT\", \"FAULT\", \"Nombre d'occurrences récentes\", \"Color Scale\"]\n",
    "    if \"ATA\" in df.columns:\n",
    "        columns_to_return.insert(2, \"ATA\")\n",
    "    df = df[columns_to_return].drop_duplicates().sort_values(by=\"Nombre d'occurrences récentes\", ascending=False)\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTIONS DE PLOTTING ET STYLE\n",
    "# =============================================================================\n",
    "def get_contrasting_text_color(color_str):\n",
    "    if color_str.startswith('#'):\n",
    "        hex_color = color_str.lstrip('#')\n",
    "        r = int(hex_color[0:2], 16)\n",
    "        g = int(hex_color[2:4], 16)\n",
    "        b = int(hex_color[4:6], 16)\n",
    "    elif color_str.startswith('rgb'):\n",
    "        inner = color_str[color_str.find('(')+1:color_str.find(')')]\n",
    "        r, g, b = [int(p.strip()) for p in inner.split(',')]\n",
    "    else:\n",
    "        r, g, b = (0, 0, 0)\n",
    "    luminance = (0.299*r + 0.587*g + 0.114*b)\n",
    "    return 'black' if luminance > 186 else 'white'\n",
    "\n",
    "def plot_timeline_with_clusters_meanshift_plotly(df, immat=None, ata=None, fault=None):\n",
    "    global df_result\n",
    "    if \"ATA\" in df.columns:\n",
    "        df_selected = df[(df[\"IMMAT\"] == immat) & (df[\"ATA\"] == ata) & (df[\"FAULT\"] == fault)]\n",
    "    else:\n",
    "        df_selected = df[(df[\"IMMAT\"] == immat) & (df[\"FAULT\"] == fault)]\n",
    "    if df_selected.empty:\n",
    "        return go.Figure(), pd.DataFrame()\n",
    "    row = df_selected.iloc[0]\n",
    "    potential_dates = pd.to_datetime(df.columns, format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "    date_cols = df.columns[potential_dates.notna()]\n",
    "    event_dates = []\n",
    "    for col in date_cols:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val != 0:\n",
    "            dt = pd.to_datetime(col, format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "            if pd.notnull(dt):\n",
    "                event_dates.append(dt)\n",
    "    if not event_dates:\n",
    "        return go.Figure(), pd.DataFrame()\n",
    "    event_dates.sort()\n",
    "    X = np.array([d.toordinal() for d in event_dates]).reshape(-1, 1)\n",
    "    bw_est = estimate_bandwidth(X, quantile=0.2, n_samples=len(X))\n",
    "    bandwidth = max(bw_est, np.std(X)/4, 1.0)\n",
    "    clusters_raw = MeanShift(bandwidth=bandwidth).fit_predict(X)\n",
    "    cluster_mapping = {}\n",
    "    clusters = []\n",
    "    current_label = 1\n",
    "    for c in clusters_raw:\n",
    "        if c not in cluster_mapping:\n",
    "            cluster_mapping[c] = current_label\n",
    "            current_label += 1\n",
    "        clusters.append(cluster_mapping[c])\n",
    "    cumulative_faults = np.arange(1, len(event_dates)+1)\n",
    "    points_by_cluster = {}\n",
    "    for d, cum, cl in zip(event_dates, cumulative_faults, clusters):\n",
    "        points_by_cluster.setdefault(cl, {\"x\": [], \"y\": []})\n",
    "        points_by_cluster[cl][\"x\"].append(d)\n",
    "        points_by_cluster[cl][\"y\"].append(cum)\n",
    "    fig = go.Figure()\n",
    "    unique_clusters = sorted(points_by_cluster.keys())\n",
    "    base_palette = px.colors.qualitative.Set1\n",
    "    for i, cl in enumerate(unique_clusters):\n",
    "        data_clust = points_by_cluster[cl]\n",
    "        color = base_palette[i % len(base_palette)]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=data_clust[\"x\"],\n",
    "            y=data_clust[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=color, size=10),\n",
    "            name=f\"Cluster {cl}\"\n",
    "        ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode='lines',\n",
    "        line=dict(color='black', dash='dot'),\n",
    "        name='Plainte MTX'\n",
    "    ))\n",
    "    fault_cleaned = str(fault).replace(\" \", \"\")\n",
    "    if \"ATA\" in df_result.columns:\n",
    "        matching_df_result = df_result[\n",
    "            (df_result[\"IMMAT\"] == immat) &\n",
    "            ((df_result[\"ATA\"] == ata) |\n",
    "             (df_result[\"FAULT\"].astype(str).str.replace(\" \", \"\").str.contains(fault_cleaned, na=False)))\n",
    "        ].copy()\n",
    "    else:\n",
    "        matching_df_result = df_result[\n",
    "            (df_result[\"IMMAT\"] == immat) &\n",
    "            (df_result[\"FAULT\"].astype(str).str.replace(\" \", \"\").str.contains(fault_cleaned, na=False))\n",
    "        ].copy()\n",
    "    matching_df_result['FoundDateDT'] = pd.to_datetime(matching_df_result[\"FoundDate\"],\n",
    "                                                        format=\"%d/%m/%Y %H:%M\", errors='coerce')\n",
    "    matching_df_result = matching_df_result.sort_values(by=\"FoundDateDT\", ascending=False).reset_index(drop=True)\n",
    "    matching_df_result.drop(columns=\"FoundDateDT\", inplace=True)\n",
    "    color_palette = px.colors.qualitative.Set1\n",
    "    matching_df_result[\"LineColor\"] = [color_palette[i % len(color_palette)] for i in range(len(matching_df_result))]\n",
    "    for idx, row_res in matching_df_result.iterrows():\n",
    "        fd = pd.to_datetime(row_res[\"FoundDate\"], format=\"%d/%m/%Y %H:%M\", errors='coerce')\n",
    "        if pd.notna(fd):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[fd, fd],\n",
    "                y=[0, max(cumulative_faults)],\n",
    "                mode=\"lines+markers\",\n",
    "                line=dict(color=row_res[\"LineColor\"], dash=\"dot\"),\n",
    "                marker=dict(size=20, opacity=0),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=f\"Date: {fd.strftime('%d/%m/%Y %H:%M')}\",\n",
    "                showlegend=False\n",
    "            ))\n",
    "    fig.update_layout(\n",
    "        title=\"Timeline des occurrences\",\n",
    "        yaxis_title=\"Numéro de l'occurrence\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig.update_xaxes(type=\"date\")\n",
    "    table_df = matching_df_result[[\"FAULT\", \"FoundDate\", \"Tasks_Barcode\"]]\n",
    "    return fig, table_df\n",
    "\n",
    "def generate_row_styles(data):\n",
    "    styles = []\n",
    "    # On détermine quelles sont les colonnes de df_final correspondant à des dates\n",
    "    potential_dates = pd.to_datetime(df_final.columns, errors='coerce')\n",
    "    date_cols = [col for col, dt in zip(df_final.columns, potential_dates) if pd.notnull(dt)]\n",
    "    \n",
    "    for i, row in enumerate(data):\n",
    "        # Couleur de fond basée sur le score déjà calculé (\"Color Scale\")\n",
    "        intensity = int(255*(1-row[\"Color Scale\"]))\n",
    "        bg_color = f\"rgb(255, {intensity}, {intensity})\"\n",
    "        \n",
    "        # Style de base pour la ligne\n",
    "        row_style = {\n",
    "            \"if\": {\"row_index\": i},\n",
    "            \"backgroundColor\": bg_color,\n",
    "            \"color\": \"black\"\n",
    "        }\n",
    "        \n",
    "        # Récupérer les clés pour filtrer df_final et df_result (IMMAT, FAULT et ATA éventuellement)\n",
    "        immat = row[\"IMMAT\"]\n",
    "        fault = row[\"FAULT\"]\n",
    "        # Vérifier si ATA existe dans la ligne (pour Airbus par exemple)\n",
    "        ata = row.get(\"ATA\", None)\n",
    "        \n",
    "        # Filtrage dans df_final (les dates du plot)\n",
    "        if ata is not None:\n",
    "            matching_rows_final = df_final[(df_final[\"IMMAT\"] == immat) &\n",
    "                                           (df_final[\"FAULT\"] == fault) &\n",
    "                                           (df_final[\"ATA\"] == ata)]\n",
    "        else:\n",
    "            matching_rows_final = df_final[(df_final[\"IMMAT\"] == immat) &\n",
    "                                           (df_final[\"FAULT\"] == fault)]\n",
    "        \n",
    "        last_event_date = None\n",
    "        if not matching_rows_final.empty:\n",
    "            # Parcourir les lignes correspondantes pour extraire les dates (colonnes de date non nulles et non zéro)\n",
    "            event_dates = []\n",
    "            for idx, row_final in matching_rows_final.iterrows():\n",
    "                for col in date_cols:\n",
    "                    # On considère que la valeur non nulle de la cellule signifie la présence d’un événement\n",
    "                    if row_final[col] != 0:\n",
    "                        # La colonne représente une date (cf. generate_date_columns)\n",
    "                        try:\n",
    "                            dt = pd.to_datetime(col, errors='coerce')\n",
    "                            if pd.notnull(dt):\n",
    "                                event_dates.append(dt)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            if event_dates:\n",
    "                last_event_date = max(event_dates)\n",
    "        \n",
    "        # Filtrage dans df_result (les plaintes MTX)  \n",
    "        if ata is not None:\n",
    "            matching_rows_result = df_result[(df_result[\"IMMAT\"] == immat) &\n",
    "                                             ((df_result[\"ATA\"] == ata) |\n",
    "                                              (df_result[\"FAULT\"].astype(str).str.replace(\" \", \"\").str.contains(str(fault).replace(\" \", \"\"), na=False)))]\n",
    "        else:\n",
    "            matching_rows_result = df_result[(df_result[\"IMMAT\"] == immat) &\n",
    "                                             (df_result[\"FAULT\"].astype(str).str.replace(\" \", \"\").str.contains(str(fault).replace(\" \", \"\"), na=False))]\n",
    "        \n",
    "        last_found_date = None\n",
    "        if not matching_rows_result.empty:\n",
    "            # Convertir les FoundDate en datetime pour pouvoir comparer\n",
    "            matching_rows_result = matching_rows_result.copy()\n",
    "            matching_rows_result['FoundDateDT'] = pd.to_datetime(matching_rows_result[\"FoundDate\"],\n",
    "                                                                  format=\"%d/%m/%Y %H:%M\", errors='coerce')\n",
    "            valid_dates = matching_rows_result['FoundDateDT'].dropna()\n",
    "            if not valid_dates.empty:\n",
    "                last_found_date = valid_dates.max()\n",
    "        \n",
    "        # Si des lignes de df_result existent ET que la dernière date du plot est supérieure à la dernière FoundDate,\n",
    "        # on ajoute une bordure rouge.\n",
    "        if last_event_date and last_found_date and last_event_date > last_found_date:\n",
    "            row_style[\"border\"] = \"2px solid red\"\n",
    "        \n",
    "        styles.append(row_style)\n",
    "    return styles\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# INTERFACE DE CHARGEMENT DES DONNÉES (UPLOAD) - MODIFIÉE\n",
    "# =============================================================================\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Détection de pannes récurrentes\"),\n",
    "    html.Div([\n",
    "        # Colonne pour le type d'avion\n",
    "        html.Div([\n",
    "            html.Label(\"Type d'avion\"),\n",
    "            dcc.Dropdown(\n",
    "                id='aircraft-type',\n",
    "                options=[\n",
    "                    {'label': 'A330', 'value': 'A330'},\n",
    "                    {'label': 'A320', 'value': 'A320'},\n",
    "                    {'label': 'B777', 'value': 'B777'}\n",
    "                ],\n",
    "                value=None,\n",
    "                placeholder=\"Sélectionner un type d'avion\"\n",
    "            )\n",
    "        ], style={'flex': '1', 'padding': '10px'}),\n",
    "        # Colonne pour le fichier ECAM\n",
    "        html.Div([\n",
    "            html.Label(\"Fichier ECAM (Excel)\"),\n",
    "            dcc.Upload(\n",
    "                id='upload-ecam',\n",
    "                children=html.Div([\n",
    "                    'Glisser un fichier ici ou cliquer pour téléverser'\n",
    "                ]),\n",
    "                style={\n",
    "                    'width': '100%',\n",
    "                    'height': '60px',\n",
    "                    'lineHeight': '60px',\n",
    "                    'borderWidth': '1px',\n",
    "                    'borderStyle': 'dashed',\n",
    "                    'borderRadius': '5px',\n",
    "                    'textAlign': 'center',\n",
    "                    'whiteSpace': 'normal',\n",
    "                    'wordWrap': 'break-word',\n",
    "                    'margin': '10px 0'\n",
    "                },\n",
    "                multiple=False\n",
    "            ),\n",
    "            html.Div(id='ecam-status', style={'margin': '5px 0'})\n",
    "        ], style={'flex': '1', 'padding': '10px'}),\n",
    "        # Colonne pour le fichier MTX\n",
    "        html.Div([\n",
    "            html.Label(\"Fichier MTX (Excel)\"),\n",
    "            dcc.Upload(\n",
    "                id='upload-mtx',\n",
    "                children=html.Div([\n",
    "                    'Glisser un fichier ici ou cliquer pour téléverser'\n",
    "                ]),\n",
    "                style={\n",
    "                    'width': '100%',\n",
    "                    'height': '60px',\n",
    "                    'lineHeight': '60px',\n",
    "                    'borderWidth': '1px',\n",
    "                    'borderStyle': 'dashed',\n",
    "                    'borderRadius': '5px',\n",
    "                    'textAlign': 'center',\n",
    "                    'whiteSpace': 'normal',\n",
    "                    'wordWrap': 'break-word',\n",
    "                    'margin': '10px 0'\n",
    "                },\n",
    "                multiple=False\n",
    "            ),\n",
    "            html.Div(id='mtx-status', style={'margin': '5px 0'})\n",
    "        ], style={'flex': '1', 'padding': '10px'}),\n",
    "    ], style={\n",
    "        'display': 'flex',\n",
    "        'flexDirection': 'row',\n",
    "        'justifyContent': 'space-between',\n",
    "        'alignItems': 'center'\n",
    "    }),\n",
    "    # Bouton d'analyse centré juste en dessous\n",
    "    html.Div([\n",
    "        html.Button(\"Lancer l'analyse\", id=\"launch-analysis\", n_clicks=0,\n",
    "                    style={'padding': '10px 20px', 'fontSize': '16px'})\n",
    "    ], style={'display': 'flex', 'justifyContent': 'center', 'padding': '10px 0'}),\n",
    "    # Conteneur du dashboard enveloppé dans un composant dcc.Loading\n",
    "    dcc.Loading(\n",
    "       id=\"loading-animation\",\n",
    "       type=\"default\",  # Vous pouvez choisir \"cube\", \"circle\", etc.\n",
    "       children=[html.Div(id='dashboard-container')]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "def parse_contents(contents):\n",
    "    content_type, content_string = contents.split(',')\n",
    "    decoded = base64.b64decode(content_string)\n",
    "    return io.BytesIO(decoded)\n",
    "\n",
    "# =============================================================================\n",
    "# CALLBACKS POUR CONFIRMER LE TÉLÉVERSEMENT DES FICHIERS\n",
    "# =============================================================================\n",
    "@app.callback(\n",
    "    Output('ecam-status', 'children'),\n",
    "    Output('upload-ecam', 'style'),\n",
    "    Input('upload-ecam', 'contents'),\n",
    "    State('upload-ecam', 'filename')\n",
    ")\n",
    "def update_ecam_status(contents, filename):\n",
    "    if contents is not None and filename is not None:\n",
    "        style = {\n",
    "            'width': '100%', 'height': '60px', 'lineHeight': '60px',\n",
    "            'borderWidth': '1px', 'borderStyle': 'solid', 'borderRadius': '5px',\n",
    "            'textAlign': 'center', 'margin': '10px 0', 'backgroundColor': '#d4edda'\n",
    "        }\n",
    "        return f\"Fichier reçu: {filename}\", style\n",
    "    else:\n",
    "        style = {\n",
    "            'width': '100%', 'height': '60px', 'lineHeight': '60px',\n",
    "            'borderWidth': '1px', 'borderStyle': 'dashed', 'borderRadius': '5px',\n",
    "            'textAlign': 'center', 'margin': '10px 0'\n",
    "        }\n",
    "        return \"\", style\n",
    "\n",
    "@app.callback(\n",
    "    Output('mtx-status', 'children'),\n",
    "    Output('upload-mtx', 'style'),\n",
    "    Input('upload-mtx', 'contents'),\n",
    "    State('upload-mtx', 'filename')\n",
    ")\n",
    "def update_mtx_status(contents, filename):\n",
    "    if contents is not None and filename is not None:\n",
    "        style = {\n",
    "            'width': '100%', 'height': '60px', 'lineHeight': '60px',\n",
    "            'borderWidth': '1px', 'borderStyle': 'solid', 'borderRadius': '5px',\n",
    "            'textAlign': 'center', 'margin': '10px 0', 'backgroundColor': '#d4edda'\n",
    "        }\n",
    "        return f\"Fichier reçu: {filename}\", style\n",
    "    else:\n",
    "        style = {\n",
    "            'width': '100%', 'height': '60px', 'lineHeight': '60px',\n",
    "            'borderWidth': '1px', 'borderStyle': 'dashed', 'borderRadius': '5px',\n",
    "            'textAlign': 'center', 'margin': '10px 0'\n",
    "        }\n",
    "        return \"\", style\n",
    "\n",
    "# =============================================================================\n",
    "# CALLBACK DE TRAITEMENT DES FICHIERS ET AFFICHAGE DU DASHBOARD\n",
    "# =============================================================================\n",
    "@app.callback(\n",
    "    Output('dashboard-container', 'children'),\n",
    "    [Input('launch-analysis', 'n_clicks')],\n",
    "    [State('aircraft-type', 'value'),\n",
    "     State('upload-ecam', 'contents'),\n",
    "     State('upload-mtx', 'contents')]\n",
    ")\n",
    "def handle_files(n_clicks, aircraft_type, ecam_contents, mtx_contents):\n",
    "    if n_clicks == 0 or not aircraft_type or not ecam_contents or not mtx_contents:\n",
    "        return html.Div(\"\")\n",
    "    \n",
    "    global df, df_result, df_final, filtered_df\n",
    "\n",
    "    ecam_file = parse_contents(ecam_contents)\n",
    "    mtx_file = parse_contents(mtx_contents)\n",
    "\n",
    "    if aircraft_type in [\"A330\", \"A320\"]:\n",
    "        df_60J, df_360J = process_airbus_excel_new_format(ecam_file)\n",
    "        df_combined = merge_60j_into_360j(df_60J, df_360J)\n",
    "    else:\n",
    "        df_60J, df_360J = process_boeing_excel_new_format(ecam_file)\n",
    "        df_combined = merge_60j_into_360j_boeing(df_60J, df_360J)\n",
    "    \n",
    "    df = df_combined.drop_duplicates()\n",
    "    potential_dates = pd.to_datetime(df.columns, errors='coerce')\n",
    "    date_cols = df.columns[potential_dates.notna()]\n",
    "    df_filtre = df[(df[date_cols].ne(0).sum(axis=1) >= 3) & (df[\"USEFUL\"] == True)]\n",
    "    df_final = filter_rows_by_mean_gap_dates(df_filtre, facteur=2)\n",
    "\n",
    "    df_mtx = pd.read_excel(mtx_file, sheet_name=\"Data\")\n",
    "    for col in df_mtx.select_dtypes(include=['object']):\n",
    "        df_mtx[col] = df_mtx[col].str.strip()\n",
    "    df_mtx[\"ATA\"] = pd.to_numeric(df_mtx[\"ATA\"], errors=\"coerce\")\n",
    "    df_mtx[\"ATA\"] = df_mtx[\"ATA\"].astype(\"Int64\")\n",
    "    df_mtx = df_mtx.rename(columns={\"Tasks_Aircraft\": \"IMMAT\", \"Task_Name_Only\": \"FAULT\"})\n",
    "    df_result = df_mtx.copy()\n",
    "\n",
    "    filtered_df = calculate_recent_occurrences(df_final)\n",
    "\n",
    "    dashboard_layout = html.Div([\n",
    "        html.H2(\"Dashboard chargé\"),\n",
    "        html.Div([\n",
    "            html.Div([\n",
    "                html.Details([\n",
    "                    html.Summary(\"Sélectionner les faults\"),\n",
    "                    # Par défaut le menu n'est pas déroulé\n",
    "                    dcc.Checklist(\n",
    "                        id=\"remove-faults-option\",\n",
    "                        options=[{\"label\": \"Retirer les pannes concernant plus de 3 avions\", \"value\": \"remove\"}],\n",
    "                        value=[\"remove\"],\n",
    "                        labelStyle={\"display\": \"block\"}\n",
    "                    ),\n",
    "                    html.Div([\n",
    "                        html.Button(\"Tout cocher\", id=\"select-all\", n_clicks=0),\n",
    "                        html.Button(\"Tout décocher\", id=\"deselect-all\", n_clicks=0)\n",
    "                    ], style={'display': 'flex', 'gap': '10px', 'margin': '5px 0'}),\n",
    "                    dcc.Checklist(\n",
    "                        id=\"fault-filter\",\n",
    "                        options=[{\"label\": f, \"value\": f} for f in sorted(filtered_df[\"FAULT\"].unique())],\n",
    "                        value=sorted(filtered_df[\"FAULT\"].unique()),\n",
    "                        labelStyle={\"display\": \"block\"}\n",
    "                    )\n",
    "                ], open=False),\n",
    "                dash_table.DataTable(\n",
    "                    id='table',\n",
    "                    columns=[{'name': c, 'id': c} for c in filtered_df.columns if c != \"Color Scale\"],\n",
    "                    data=filtered_df.to_dict('records'),\n",
    "                    filter_action='native',\n",
    "                    sort_action='native',\n",
    "                    row_selectable='single',\n",
    "                    style_table={'overflowX': 'auto', 'maxHeight': '600px', 'overflowY': 'scroll'},\n",
    "                    style_data_conditional=generate_row_styles(filtered_df.to_dict('records'))\n",
    "                )\n",
    "            ], style={'width': '30%', 'padding': '10px', 'overflowY': 'auto'}),\n",
    "            html.Div([\n",
    "                dcc.Graph(id='timeline-plot'),\n",
    "                html.Hr(),\n",
    "                html.H3(\"Plaintes MTX\"),\n",
    "                dash_table.DataTable(\n",
    "                    id='df-result-table',\n",
    "                    columns=[{'name': c, 'id': c} for c in [\"FAULT\", \"FoundDate\", \"Tasks_Barcode\"]],\n",
    "                    data=[],\n",
    "                    style_table={'overflowX': 'auto', 'maxHeight': '300px', 'overflowY': 'scroll'},\n",
    "                    style_cell={'textAlign': 'left'},\n",
    "                )\n",
    "            ], style={'width': '70%', 'padding': '10px'})\n",
    "        ], style={'display': 'flex', 'flexDirection': 'row'})\n",
    "    ])\n",
    "    return dashboard_layout\n",
    "\n",
    "# =============================================================================\n",
    "# CALLBACKS DU DASHBOARD (Mise à jour table, graphique, etc.)\n",
    "# =============================================================================\n",
    "@app.callback(\n",
    "    [Output('table', 'data'),\n",
    "     Output('table', 'style_data_conditional'),\n",
    "     Output('fault-filter', 'options'),\n",
    "     Output('fault-filter', 'value')],\n",
    "    [Input('fault-filter', 'value'),\n",
    "     Input('remove-faults-option', 'value'),\n",
    "     Input('select-all', 'n_clicks'),\n",
    "     Input('deselect-all', 'n_clicks')],\n",
    "    State('fault-filter', 'options')\n",
    ")\n",
    "def update_table_and_faults(selected_faults, remove_option, select_all, deselect_all, current_options):\n",
    "    ctx = dash.callback_context\n",
    "    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None\n",
    "    df_table = filtered_df.copy()\n",
    "    if \"remove\" in (remove_option or []):\n",
    "        fault_counts = df_table.groupby(\"FAULT\")[\"IMMAT\"].nunique()\n",
    "        faults_to_exclude = fault_counts[fault_counts > 3].index\n",
    "        df_table = df_table[~df_table[\"FAULT\"].isin(faults_to_exclude)]\n",
    "    available_faults = sorted(df_table[\"FAULT\"].unique())\n",
    "    new_options = [{\"label\": f, \"value\": f} for f in available_faults]\n",
    "    if trigger_id == 'select-all':\n",
    "        new_selected = available_faults\n",
    "    elif trigger_id == 'deselect-all':\n",
    "        new_selected = []\n",
    "    else:\n",
    "        new_selected = [f for f in (selected_faults or []) if f in available_faults]\n",
    "        if not new_selected:\n",
    "            new_selected = available_faults\n",
    "    if new_selected:\n",
    "        df_table = df_table[df_table[\"FAULT\"].isin(new_selected)]\n",
    "    else:\n",
    "        df_table = pd.DataFrame(columns=filtered_df.columns)\n",
    "    records = df_table.to_dict('records')\n",
    "    new_styles = generate_row_styles(records)\n",
    "    return records, new_styles, new_options, new_selected\n",
    "\n",
    "@app.callback(\n",
    "    [Output('timeline-plot', 'figure'),\n",
    "     Output('df-result-table', 'data'),\n",
    "     Output('df-result-table', 'style_data_conditional')],\n",
    "    [Input('table', 'selected_rows'),\n",
    "     Input('timeline-plot', 'restyleData')],\n",
    "    [State('table', 'data'),\n",
    "     State('timeline-plot', 'figure')]\n",
    ")\n",
    "def display_plot(selected_rows, restyleData, table_data, current_fig):\n",
    "    ctx = dash.callback_context\n",
    "    if not ctx.triggered:\n",
    "        raise dash.exceptions.PreventUpdate\n",
    "    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0]\n",
    "    if trigger_id == 'table':\n",
    "        if not selected_rows:\n",
    "            return go.Figure(), [], []\n",
    "        selected_row = table_data[selected_rows[0]]\n",
    "        immat = selected_row[\"IMMAT\"]\n",
    "        ata = selected_row.get(\"ATA\", None)\n",
    "        fault = selected_row[\"FAULT\"]\n",
    "        new_fig, result_df = plot_timeline_with_clusters_meanshift_plotly(df_final, immat=immat, ata=ata, fault=fault)\n",
    "        result_df['FoundDateDT'] = pd.to_datetime(result_df['FoundDate'], format=\"%d/%m/%Y %H:%M\", errors='coerce')\n",
    "        result_df = result_df.sort_values(by='FoundDateDT', ascending=False)\n",
    "        result_df.drop(columns='FoundDateDT', inplace=True)\n",
    "        table_result_data = result_df.to_dict('records')\n",
    "        style_result = []\n",
    "        color_palette = px.colors.qualitative.Set1\n",
    "        for i in range(len(result_df)):\n",
    "            bg = color_palette[i % len(color_palette)]\n",
    "            txt = get_contrasting_text_color(bg)\n",
    "            style_result.append({\n",
    "                'if': {'row_index': i},\n",
    "                'backgroundColor': bg,\n",
    "                'color': txt\n",
    "            })\n",
    "        return new_fig, table_result_data, style_result\n",
    "    elif trigger_id == 'timeline-plot' and restyleData is not None:\n",
    "        update_info = restyleData[0]\n",
    "        trace_indices = restyleData[1]\n",
    "        if 'visible' in update_info:\n",
    "            new_visible = update_info['visible'][0]\n",
    "            if new_visible != 'legendonly':\n",
    "                idx = trace_indices[0]\n",
    "                if current_fig and 'data' in current_fig and len(current_fig['data']) > idx:\n",
    "                    x_data = current_fig['data'][idx].get('x', [])\n",
    "                    parsed_dates = []\n",
    "                    for val in x_data:\n",
    "                        try:\n",
    "                            dt = parse(val)\n",
    "                            parsed_dates.append(dt)\n",
    "                        except:\n",
    "                            pass\n",
    "                    if parsed_dates:\n",
    "                        x_min = min(parsed_dates)\n",
    "                        x_max = max(parsed_dates)\n",
    "                        delta_days = (x_max - x_min).days\n",
    "                        x_margin_days = delta_days * 0.1 if delta_days > 0 else 1\n",
    "                        current_fig['layout']['xaxis']['range'] = [\n",
    "                            (x_min - pd.Timedelta(days=x_margin_days)).isoformat(),\n",
    "                            (x_max + pd.Timedelta(days=x_margin_days)).isoformat()\n",
    "                        ]\n",
    "        return current_fig, dash.no_update, dash.no_update\n",
    "    else:\n",
    "        return current_fig, dash.no_update, dash.no_update\n",
    "\n",
    "# =============================================================================\n",
    "# LANCEMENT DE L'APPLICATION\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce7254-0ac4-4ae8-a421-364305501a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
